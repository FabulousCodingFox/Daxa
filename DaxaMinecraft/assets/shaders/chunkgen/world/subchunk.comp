#version 450
#extension GL_KHR_vulkan_glsl : enable
#extension GL_KHR_shader_subgroup : enable
#extension GL_KHR_shader_subgroup_arithmetic : enable
#extension GL_KHR_shader_subgroup_basic : enable
#extension GL_EXT_debug_printf : enable

layout(local_size_x = 1024, local_size_y = 1, local_size_z = 1) in;

#include <chunkgen/common.glsl>

shared uint local_copy_x2[1024];
shared uint local_copy_x4[128];
shared uint local_copy_x8[16];
shared uint local_copy_x16[2];

// 2 4

#define X4_GENERATION_USE_X2_LOCAL_COPY

#define X16_GENERATION_USE_X8_LOCAL_COPY

void generate_x2(in uvec3 chunk_i) {
    for (int n = 0; n < 32; n++) {
        uvec3 x2_i = linear_index_to_x2_packed_index(gl_GlobalInvocationID.x + 1024 * n);
        uvec3 world_i = uvec3(p.pos) + x2_i * 2;
        bool at_least_one_occluding = false;
        for (int x = 0; x < 2; x++) {
            for (int y = 0; y < 2; y++) {
                for (int z = 0; z < 2; z++) {
                    vec3 world_pos_sub = world_i + uvec3(x, y, z);
                    uint block_id = load_block_id(world_pos_sub);
                    at_least_one_occluding = at_least_one_occluding || is_block_occluding(block_id);
                }
            }
        }
        uint result = 0x00000000;
        if (at_least_one_occluding) {
            result = x2_uint_bit_mask(x2_i);
        }
        subgroupBarrier();
        uint subgroup32_result_packed = subgroupOr(result);
        subgroupBarrier();
        if (subgroupElect()) {
            uint index = x2_uint_array_index(x2_i);
            chunk_block_presence(chunk_i).x2[index] = subgroup32_result_packed;
            local_copy_x2[index] = subgroup32_result_packed;
        }
    }
}

void generate_x4(in uvec3 chunk_i) {
    for (int n = 0; n < 4; n++) {
        uint virtual_invocation = gl_GlobalInvocationID.x + n * 1024;
        uvec3 x4_i = linear_index_to_x4_packed_index(virtual_invocation);
        uvec3 world_i = uvec3(p.pos) + x4_i * 4;
        bool at_least_one_occluding = false;
#ifdef X4_GENERATION_USE_X2_LOCAL_COPY
        for (int x = 0; x < 2; x++) {
            for (int y = 0; y < 2; y++) {
                for (int z = 0; z < 2; z++) {
                    uvec3 x2_i = x4_i * 2 + uvec3(x, y, z);
                    uint index = x2_uint_array_index(x2_i);
                    uint mask = x2_uint_bit_mask(x2_i);
                    bool present = (local_copy_x2[index] & mask) != 0;
                    at_least_one_occluding = at_least_one_occluding || present;
                }
            }
        }
#else
        for (int x = 0; x < 4; x++) {
            for (int y = 0; y < 4; y++) {
                for (int z = 0; z < 4; z++) {
                    vec3 world_pos_sub = world_i + uvec3(x, y, z);
                    uint block_id = load_block_id(world_pos_sub);
                    at_least_one_occluding = at_least_one_occluding || is_block_occluding(block_id);
                }
            }
        }
#endif
        uint result = 0x00000000;
        if (at_least_one_occluding) {
            result = x4_uint_bit_mask(x4_i);
        }
        subgroupBarrier();
        uint subgroup32_result_packed = subgroupOr(result);
        subgroupBarrier();
        if (subgroupElect()) {
            uint block_array_index = x4_uint_array_index(x4_i);
            chunk_block_presence(chunk_i).x4[block_array_index] = subgroup32_result_packed;
            local_copy_x4[block_array_index] = subgroup32_result_packed;
        }
    }
}

void generate_x8(in uvec3 chunk_i) {
    uvec3 x8_i = linear_index_to_x8_packed_index(gl_GlobalInvocationID.x);

    uvec3 world_i = uvec3(p.pos) + x8_i * 8;
    bool at_least_one_occluding = false;
    for (int x = 0; x < 2; x++) {
        for (int y = 0; y < 2; y++) {
            for (int z = 0; z < 2; z++) {
                uvec3 x4_i = x8_i * 2 + uvec3(x, y, z);
                uint index = x4_uint_array_index(x4_i);
                uint mask = x4_uint_bit_mask(x4_i);
                bool present = (local_copy_x4[index] & mask) != 0;
                at_least_one_occluding = at_least_one_occluding || present;
            }
        }
    }
    uint result = 0x00000000;
    if (at_least_one_occluding) {
        result = x8_uint_bit_mask(x8_i);
    }
    subgroupBarrier();
    uint subgroup32_result_packed = subgroupOr(result);
    subgroupBarrier();
    if (subgroupElect()) {
        uint index = x8_uint_array_index(x8_i);
        chunk_block_presence(chunk_i).x8[index] = subgroup32_result_packed;
        local_copy_x8[index] = subgroup32_result_packed;
    }
}

void generate_x16(in uvec3 chunk_i) {
    uvec3 x16_i = uvec3(
        gl_GlobalInvocationID.x & 0x3,
        (gl_GlobalInvocationID.x >> 2) & 0x3,
        (gl_GlobalInvocationID.x >> 4) & 0x3);

    uvec3 x16_i_4 = uvec3(
        x16_i.x * 4,
        x16_i.y * 4,
        x16_i.z * 4);
    bool at_least_one_occluding = false;
#ifdef X16_GENERATION_USE_X8_LOCAL_COPY
    for (int x = 0; x < 2; x++) {
        for (int y = 0; y < 2; y++) {
            for (int z = 0; z < 2; z++) {
                uvec3 x8_i = x16_i * 2 + uvec3(x, y, z);
                uint index = x8_uint_array_index(x8_i);
                uint mask = x8_uint_bit_mask(x8_i);
                at_least_one_occluding =
                    at_least_one_occluding || (local_copy_x8[index] & mask) != 0;
            }
        }
    }
#else
#ifdef X4_444_PACKING

    uint x4_array_index0 = x4_uint_array_index(x16_i_4);

    at_least_one_occluding = (local_copy_x4[x4_array_index0] |
                              local_copy_x4[x4_array_index0 + 1]) != 0;
#else
    for (int x = 0; x < 4; x++) {
        for (int y = 0; y < 4; y++) {
            for (int z = 0; z < 4; z++) {
                uvec3 x4_i = x16_i_4 + uvec3(x, y, z);
                uint index = x4_uint_array_index(x4_i);
                uint mask = x4_uint_bit_mask(x4_i);
                at_least_one_occluding =
                    at_least_one_occluding || (local_copy_x4[index] & mask) != 0;
            }
        }
    }
#endif
#endif
    uint result = 0;
    if (at_least_one_occluding) {
        result = x16_uint_bit_mask(x16_i);
    }
    subgroupBarrier();
    uint subgroup32_result_packed = subgroupOr(result);
    subgroupBarrier();
    if (subgroupElect()) {
        uint uint_index_x16 = x16_uint_array_index(x16_i);
        chunk_block_presence(chunk_i).x16[uint_index_x16] = subgroup32_result_packed;
        local_copy_x16[uint_index_x16] = subgroup32_result_packed;
    }
}

void generate_x32(in uvec3 chunk_i) {
    uvec3 x32_i = uvec3(
        gl_GlobalInvocationID.x & 0x1,
        (gl_GlobalInvocationID.x >> 1) & 0x1,
        (gl_GlobalInvocationID.x >> 2) & 0x1);

    uvec3 x32_i_2 = uvec3(
        x32_i.x * 2,
        x32_i.y * 2,
        x32_i.z * 2);
    uint x32_index = x32_uint_array_index(x32_i);

    bool at_least_one_occluding = false;
    for (int x = 0; x < 2; x++) {
        for (int y = 0; y < 2; y++) {
            for (int z = 0; z < 2; z++) {
                uvec3 x16_i = x32_i_2 + uvec3(x, y, z);
                uint index = x16_uint_array_index(x16_i);
                uint mask = x16_uint_bit_mask(x16_i);
                at_least_one_occluding =
                    at_least_one_occluding || (local_copy_x16[index] & mask) != 0;
            }
        }
    }

    chunk_block_presence(chunk_i).x32[x32_index] = at_least_one_occluding;
}

void main() {
    uvec3 chunk_i = uvec3(p.pos) / CHUNK_SIZE;

    generate_x2(chunk_i);
    memoryBarrierShared();

    generate_x4(chunk_i);
    memoryBarrierShared();

    // as only 512 threads are needed for x8,
    // all other threads will be terminated so that the SM
    // can execute other warps in the meantime
    if (gl_GlobalInvocationID.x >= 512) {
        return;
    }
    generate_x8(chunk_i);
    memoryBarrierShared();

    // As the x16 and higher only need 64 threads,
    // all threads with an id higher than 63 are terminated
    if (gl_GlobalInvocationID.x >= 64) {
        return;
    }
    generate_x16(chunk_i);
    memoryBarrierShared();

    // end all threads except for 8,
    // as x32 only requires 8 threads
    if (gl_GlobalInvocationID.x >= 8) {
        return;
    }
    generate_x32(chunk_i);
}
